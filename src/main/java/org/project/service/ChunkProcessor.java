package org.project.service;

import org.project.utils.JsonUtils;
import java.io.*;
import java.util.*;

public class ChunkProcessor {
    private final FileChunker fileChunker;
    private final Set<String> existingChunks;
    private double timeSlicingFile;
    private double timeCompressionFile;
    private final Compressor compressor;

    public ChunkProcessor(FileChunker fileChunker) {
        this.fileChunker = fileChunker;
        this.existingChunks = loadExistingChunks();
        this.compressor = new Compressor();
    }

    private Set<String> loadExistingChunks() {
        File chunksDirectory = new File("chunks");
        Set<String> chunkSet = new HashSet<>();
        if (chunksDirectory.exists()) {
            for (File file : Objects.requireNonNull(chunksDirectory.listFiles())) {
                if (file.getName().endsWith(".zst")) {
                    chunkSet.add(file.getName().replace(".zst", ""));
                }
            }
        }
        return chunkSet;
    }

    public void processFile(File file, boolean withMessage) throws IOException {
        if (withMessage) {
            System.out.println("\n=========================================");
            System.out.println(" ðŸ“‚ Traitement du fichier : " + file.getName());
            System.out.println("=========================================\n");
        }

        // ðŸ”¹ RÃ©cupÃ©rer l'extension du fichier
        String extension = "";
        int lastDotIndex = file.getName().lastIndexOf(".");
        if (lastDotIndex != -1) {
            extension = file.getName().substring(lastDotIndex);
        }

        long startTimeSlicing = System.nanoTime();
        List<byte[]> chunks = this.fileChunker.getChunks(file);
        long endTimeSlicing = System.nanoTime();
        this.timeSlicingFile = (endTimeSlicing - startTimeSlicing) / 1e6;

        List<String> chunkSequence = new ArrayList<>();
        Map<String, Integer> compressedChunkSizes = new HashMap<>();
        Map<String, String> compressedChunksData = new LinkedHashMap<>();
        int chunkCount = 1;
        int totalOriginalSize = 0;
        int totalCompressedSize = 0;
        int newChunksStored = 0;
        int duplicateChunks = 0;
        int totalOriginalChunkSize = 0;
        int totalStoredChunkSize = 0;

        File chunksDirectory = new File("chunks");
        if (!chunksDirectory.exists()) {
            chunksDirectory.mkdir();
        }

        long startTimeCompression = System.nanoTime();
        for (byte[] chunk : chunks) {
            String chunkHash = Blake3Hasher.hashChunk(chunk);
            int chunkSize = chunk.length;
            totalOriginalSize += chunkSize;
            totalOriginalChunkSize += chunkSize;

            File chunkFile = new File("chunks/" + chunkHash + ".zst");
            byte[] compressedChunk;

            if (!chunkFile.exists()) {
                // ðŸ”¹ Nouveau chunk â†’ compression et stockage
                compressedChunk = this.compressor.compressChunkWithZstd(chunk);
                totalCompressedSize += compressedChunk.length;
                compressedChunkSizes.put(chunkHash, compressedChunk.length);
                newChunksStored++;
                totalStoredChunkSize += chunkSize; // Ajoute la taille uniquement pour les chunks uniques

                try (FileOutputStream fos = new FileOutputStream(chunkFile)) {
                    fos.write(compressedChunk);
                }

                existingChunks.add(chunkHash);

                if (withMessage) {
                    System.out.println("\nðŸ“¦ Chunk " + chunkCount + " [Nouveau]");
                    System.out.println("  â—‹ Hash   : " + chunkHash);
                    System.out.println("  â—‹ Taille originale : " + chunk.length + " bytes");
                    System.out.println("  âš¡ Compression appliquÃ©e");
                    System.out.println("  âš¡ Taille compressÃ©e : " + compressedChunk.length + " bytes");
                }
            } else {
                duplicateChunks++;
                compressedChunk = readFile(chunkFile); // Charger le chunk dÃ©jÃ  existant

                if (withMessage) {
                    System.out.println("\nðŸ“¦ Chunk " + chunkCount + " [DÃ©jÃ  existant]");
                    System.out.println("  â—‹ Hash   : " + chunkHash);
                    System.out.println("  â—‹ Taille originale : " + chunk.length + " bytes");
                    System.out.println("  âœ… Chunk dÃ©jÃ  stockÃ©, rÃ©utilisation.");
                }
            }

            // ðŸ”¹ Stocker TOUS les chunks compressÃ©s du fichier actuel
            compressedChunksData.put(chunkHash, Base64.getEncoder().encodeToString(compressedChunk));

            chunkSequence.add(chunkHash);
            chunkCount++;
        }
        long endTimeCompression = System.nanoTime();
        this.timeCompressionFile = (endTimeCompression - startTimeCompression) / 1e6;

        double deduplicationRatio = ((double) duplicateChunks / (chunkSequence.isEmpty() ? 1 : chunkSequence.size())) * 100;
        double storageGain = ((double) (totalOriginalChunkSize - totalStoredChunkSize) / totalOriginalChunkSize) * 100;

        // ðŸ”¹ Sauvegarder dans `compressed_chunks.json`
        JsonUtils.saveToJsonFile(compressedChunksData, "compressed_chunks.json");

        if (withMessage) {
            File resultFile = new File("result.json");
            long compressedSize = resultFile.length(); // Taille du fichier result.json

            System.out.println("\nðŸ“Š RÃ©sumÃ© du traitement :");
            System.out.println("âœ… " + chunkSequence.size() + " chunks analysÃ©s.");
            System.out.println("âœ… " + newChunksStored + " nouveaux chunks stockÃ©s.");
            System.out.println("âœ… " + duplicateChunks + " chunks supprimÃ©s (dÃ©duplication)");
            System.out.println("âœ… Ratio de dÃ©duplication : " + String.format("%.2f", deduplicationRatio) + "%");
            System.out.println("âœ… Gain de stockage grÃ¢ce Ã  la dÃ©duplication : " + String.format("%.2f", storageGain) + "%");
            System.out.println("âœ… Taille originale : " + totalOriginalSize + " bytes");
            System.out.println("âœ… Taille compressÃ©e : " + compressedSize + " bytes");

            // ðŸ”¹ Enregistrer les mÃ©tadonnÃ©es dans result.json
            Map<String, Object> resultData = new HashMap<>();
            resultData.put("chunks", chunkSequence);
            resultData.put("extension", extension);
            resultData.put("compressedChunkSizes", compressedChunkSizes);
            JsonUtils.saveToJsonFile(resultData, "result.json");

            System.out.println("\nâœ… Fichiers de sauvegarde mis Ã  jour :");
            System.out.println("  ðŸ“„ result.json  â†’ MÃ©tadonnÃ©es de reconstruction enregistrÃ©es.");
            System.out.println("  ðŸ“„ compressed_chunks.json  â†’ DonnÃ©es compressÃ©es du fichier actuel.");
        }
    }

    private byte[] readFile(File file) throws IOException {
        try (FileInputStream fis = new FileInputStream(file);
             ByteArrayOutputStream buffer = new ByteArrayOutputStream()) {
            byte[] temp = new byte[8192];
            int bytesRead;
            while ((bytesRead = fis.read(temp)) != -1) {
                buffer.write(temp, 0, bytesRead);
            }
            return buffer.toByteArray();
        }
    }

    public double getTimeSlicingFile() {
        return this.timeSlicingFile;
    }

    public double getTimeCompressionFile() {
        return this.timeCompressionFile;
    }
}
