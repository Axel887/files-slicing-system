package org.project.service;

import org.project.utils.JsonUtils;
import org.project.storage.ChunkStorage;
import java.io.File;
import java.io.IOException;
import java.util.*;
import java.util.Base64;

public class ChunkProcessor {
    private final ChunkStorage chunkStorage;
    private final FileChunker fileChunker;
    private final Compressor compressor;

    public ChunkProcessor(ChunkStorage chunkStorage, FileChunker fileChunker) {
        this.chunkStorage = chunkStorage;
        this.fileChunker = fileChunker;
        this.compressor = new Compressor();
    }

    public void processFile(File file) throws IOException {
        System.out.println("\n=========================================");
        System.out.println(" ğŸ“‚ Traitement du fichier : " + file.getName());
        System.out.println("=========================================\n");

        // RÃ©cupÃ©rer l'extension du fichier d'origine
        String extension = "";
        int lastDotIndex = file.getName().lastIndexOf(".");
        if (lastDotIndex != -1) {
            extension = file.getName().substring(lastDotIndex); // Inclut le point (ex: ".png")
        }

        // ğŸ”¹ Charger tous les chunks existants depuis `chunks.json`
        Map<String, String> chunkMapping = JsonUtils.readFromJsonFile("chunks.json", Map.class);
        if (chunkMapping == null) {
            chunkMapping = new LinkedHashMap<>();
        }
        System.out.println("ğŸ“„ chunks.json chargÃ© : " + chunkMapping.size() + " chunks uniques existants.");

        // ğŸ”¹ CrÃ©er une liste pour la sÃ©quence des chunks du fichier en cours (`result.json`)
        List<String> chunkSequence = new ArrayList<>();

        // ğŸ”¹ DÃ©couper le fichier en chunks
        List<byte[]> chunks = fileChunker.getChunks(file);
        int chunkCount = 1; // NumÃ©ro du chunk pour affichage
        int uniqueChunksStored = 0; // Nombre de nouveaux chunks ajoutÃ©s

        for (byte[] chunk : chunks) {
            boolean isNewChunk = processChunk(chunk, chunkCount, chunkMapping, chunkSequence);
            if (isNewChunk) {
                uniqueChunksStored++;
            }
            chunkCount++;
        }

        System.out.println("\nğŸ“Š RÃ©sumÃ© du traitement :");
        System.out.println("âœ… " + chunkSequence.size() + " chunks analysÃ©s.");
        System.out.println("âœ… " + uniqueChunksStored + " nouveaux chunks stockÃ©s (hors doublons).");

        // ğŸ”¹ Sauvegarder tous les chunks dans `chunks.json` (mise Ã  jour globale)
        JsonUtils.saveToJsonFile(chunkMapping, "chunks.json");

        // ğŸ”¹ Sauvegarder la sÃ©quence des chunks et l'extension dans `result.json`
        Map<String, Object> resultData = new HashMap<>();
        resultData.put("chunks", chunkSequence);
        resultData.put("extension", extension); // Ajout de l'extension

        JsonUtils.saveToJsonFile(resultData, "result.json");

        System.out.println("\nâœ… Fichiers de sauvegarde mis Ã  jour :");
        System.out.println("  ğŸ“„ chunks.json  â†’ Contient " + chunkMapping.size() + " chunks uniques.");
        System.out.println("  ğŸ“„ result.json  â†’ Contient la sÃ©quence des chunks pour la reconstruction.");
    }

    private boolean processChunk(byte[] chunk, int chunkCount, Map<String, String> chunkMapping, List<String> chunkSequence) {
        String chunkHash = Blake3Hasher.hashChunk(chunk);
        String chunkData = Base64.getEncoder().encodeToString(chunk);

        boolean isNewChunk = false;

        System.out.println("\nğŸ“¦ Chunk " + chunkCount);
        System.out.println("  â—‹ Hash   : " + chunkHash);
        System.out.println("  â—‹ Taille : " + chunk.length + " bytes");

        // ğŸ”¹ VÃ©rifier si le chunk existe dÃ©jÃ  dans `chunks.json`
        if (!chunkMapping.containsKey(chunkHash)) {
            chunkMapping.put(chunkHash, chunkData);
            byte[] compressedChunk = compressor.compressChunkWithZstd(chunk);
            chunkStorage.storeChunk(chunkHash, compressedChunk);
            isNewChunk = true;

            System.out.println("  âš¡ Compression appliquÃ©e");
            System.out.println("  âš¡ Taille compressÃ©e : " + compressedChunk.length + " bytes");
        } else {
            System.out.println("  ğŸ” Chunk dÃ©jÃ  existant (retrouvÃ© dans chunks.json) !");
        }

        chunkSequence.add(chunkHash);
        System.out.println("-----------------------------------------");

        return isNewChunk;
    }

    public void compressChunksWithoutMessage(List<byte[]> chunks) {
        for (byte[] chunk : chunks) {
            String chunkHash = Blake3Hasher.hashChunk(chunk);
            if (!chunkStorage.contains(chunkHash)) {
                byte[] compressedChunk = compressor.compressChunkWithZstd(chunk);
                chunkStorage.storeChunk(chunkHash, compressedChunk);
            }
        }
    }

    public ChunkStorage getChunkStorage() {
        return this.chunkStorage;
    }
}
