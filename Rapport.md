# Rapport d'Analyse du Syst√®me de Chunking et Compression

## 1Ô∏è‚É£ Introduction
Ce rapport pr√©sente une analyse approfondie des performances du syst√®me de d√©coupage de fichiers (chunking) et de compression mis en place. L'analyse se base sur les donn√©es extraites de result.json, qui enregistre les m√©tadonn√©es des fichiers trait√©s :

- Extension du fichier
- Liste des chunks g√©n√©r√©s (identifi√©s par un hash unique)
- Taille compress√©e des chunks enregistr√©s

Tous les chunks g√©n√©r√©s sont stock√©s dans le dossier chunks. Lorsqu'un chunk est d√©j√† pr√©sent, il n'est pas dupliqu√©, ce qui permet d'optimiser l'espace de stockage et d'am√©liorer les performances du syst√®me. Cette gestion efficace des doublons permet une r√©duction significative de la taille totale des fichiers stock√©s, en particulier pour les fichiers contenant des donn√©es r√©p√©titives.

Nous allons comparer les r√©sultats obtenus sur diff√©rents types de fichiers afin de mettre en √©vidence les gains en stockage et performances du syst√®me.

---

## 2Ô∏è‚É£ Pr√©sentation des Donn√©es Analytiques
### üìå **Fichiers Test√©s** (Masque utilis√© : `0x1FFF`)

| Fichier              | Taille Originale | Chunks Analys√©s | Chunks Stock√©s | D√©duplication | Taille Comprim√©e | Gain Stockage |
|----------------------|-----------------|---------------|---------------|--------------|----------------|--------------|
| `test1.txt`         | 358 884 bytes   | 16            | 16            | ‚ùå 0%        | 76 bytes       | ‚ùå 0%        |
| `test2.txt`         | 374 778 bytes   | 17            | 2             | ‚úÖ 88.24%    | 2 412 bytes    | ‚úÖ 93.96%    |
| `cat.png`           | 916 149 bytes   | 38            | 38            | ‚ùå 0%        | 1 390 bytes    | ‚ùå 0%        |
| `cat_duplicate.png` | 916 149 bytes   | 38            | 0             | ‚úÖ 100%      | 5 699 bytes    | ‚úÖ 100%      |
| `test.pdf`          | 56 028 bytes    | 3             | 3             | ‚ùå 0%        | 2 659 bytes    | ‚ùå 0%        |
| `test.zip`          | 160 bytes       | 1             | 1             | ‚ùå 0%        | 129 bytes      | ‚ùå 0%        |
| `fake.log`          | 5 026 bytes     | 1             | 1             | ‚ùå 0%        | 772 bytes      | ‚ùå 0%        |
| `fakesdatas.csv`    | 4 067 bytes     | 1             | 1             | ‚ùå 0%        | 975 bytes      | ‚ùå 0%        |

### **üîç Analyse des r√©sultats :**
- **`test1.txt`** a g√©n√©r√© **16 chunks** mais **aucune d√©duplication** n'a √©t√© d√©tect√©e.
- **`test2.txt`** contient **15 chunks en double**, ce qui a permis une **r√©duction de stockage de 93,96%**.
- **`cat_duplicate.png`** est une copie exacte de `cat.png`, donc **100% des chunks ont √©t√© r√©utilis√©s**.
- Les fichiers **compress√©s (`test.zip`, `test.pdf`) n'ont pas b√©n√©fici√© de d√©duplication**, ce qui est logique puisque la compression les rend plus uniques.
- **Les fichiers logs et CSV** n'ont pas de doublons d√©tect√©s et ont subi une **compression mod√©r√©e**.

---

## 3Ô∏è‚É£ Comparaison des Performances de Compression (Masque `0x1FFF`)

| Fichier              | Compression Globale (octets) | Compression Par Chunks (octets) | Temps Compression (ms) | Temps D√©compression (ms) |
|----------------------|----------------------------|--------------------------------|------------------------|--------------------------|
| `test1.txt`         | 538                         | 2 412                          | 2.99                   | 11.68                    |
| `test2.txt`         | 5 432                       | 1 390                          | 2.05                   | 0.87                     |
| `cat.png`           | 914 929                     | 5 699                          | 4.64                   | 1.94                     |
| `cat_duplicate.png` | 914 929                     | 2 659                          | 4.53                   | 7.24                     |
| `test.pdf`          | 50 420                      | 519                            | 0.29                   | 0.56                     |
| `test.zip`          | 129                         | 222                            | 0.03                   | 0.50                     |
| `fake.log`          | 772                         | 222                            | 0.06                   | 0.45                     |
| `fakesdatas.csv`    | 975                         | 222                            | 0.06                   | 0.45                     |

---

## 4Ô∏è‚É£ Comparaison des Performances avec d'autres Masques

Afin d'affiner l'analyse, les performances de d√©coupage et compression ont √©t√© compar√©es avec d'autres masques (`0x3FFF` et `0x0FFF`). Voici les principales diff√©rences observ√©es :

| Masque  | Nombre de Chunks | Ratio Compression (%) | Temps Compression (ms) | Temps D√©compression (ms) |
|---------|----------------|----------------------|----------------------|----------------------|
| `0x1FFF` | 16             | **99.85%**           | 2.99                 | 11.69                |
| `0x3FFF` | 17             | **98.55%**           | 2.04                 | 0.87                 |
| `0x0FFF` | 19             | **99.85%**           | 2.74                 | 10.38                |

### **üîç Analyse comparative :**
- **`0x1FFF` offre le meilleur √©quilibre** entre le nombre de chunks g√©n√©r√©s, la compression et la rapidit√© de traitement.
- **`0x3FFF` est plus rapide** en compression et d√©compression mais sacrifie l√©g√®rement le ratio de compression.
- **`0x0FFF` g√©n√®re plus de chunks**, ce qui peut entra√Æner un surco√ªt de gestion tout en conservant un bon taux de compression.

---


